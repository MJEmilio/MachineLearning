{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "- The covariance matrix can tell us about the scatter within a dataset, which is the amount of spread that there is within the data. The way to find this scatter is to multiply\n",
    "the covariance by the $p_c$ , the probability of the class (that is, the number of datapoints\n",
    "there are in that class divided by the total number). Adding the values of this for all of the\n",
    "classes gives us a measure of the **within-class scatter** of the dataset:\n",
    "\n",
    "\\begin{equation}\n",
    "S_W = \\sum_{c \\in classes} \\sum_{j \\in c} p_c(x_j - \\mu_c)(x_j - \\mu_c)^T .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sw: \n",
      " [[0.04141633 0.03307211 0.0054517  0.00344354]\n",
      " [0.03307211 0.0478966  0.00389932 0.00309932]\n",
      " [0.0054517  0.00389932 0.01005306 0.00202313]\n",
      " [0.00344354 0.00309932 0.00202313 0.00370204]]\n",
      "Sw: \n",
      " [[0.13022721 0.06146667 0.06641769 0.02203673]\n",
      " [0.06146667 0.08071973 0.03145034 0.01683401]\n",
      " [0.06641769 0.03145034 0.0836585  0.02639048]\n",
      " [0.02203673 0.01683401 0.02639048 0.01673741]]\n",
      "Sw: \n",
      " [[0.26500816 0.09272109 0.16751429 0.03840136]\n",
      " [0.09272109 0.11538776 0.05524354 0.0327102 ]\n",
      " [0.16751429 0.05524354 0.18518776 0.04266531]\n",
      " [0.03840136 0.0327102  0.04266531 0.04188163]]\n",
      "sb\n",
      " [[ 0.42141422 -0.13301778  1.101656    0.47519556]\n",
      " [-0.13301778  0.07563289 -0.38159733 -0.15288444]\n",
      " [ 1.101656   -0.38159733  2.91401867  1.24516   ]\n",
      " [ 0.47519556 -0.15288444  1.24516     0.53608889]] \n",
      "\n",
      "[[ 0.42068535 -0.13515509  1.10680115  0.47786933]\n",
      " [-0.13515509  0.07459166 -0.38489991 -0.15434958]\n",
      " [ 1.10680115 -0.38489991  2.9310901   1.25294409]\n",
      " [ 0.47786933 -0.15434958  1.25294409  0.53912463]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "target_ids = np.unique(labels)#labels = array([0, 1, 2])\n",
    "Ndata = np.shape(data)[0]\n",
    "Sw = np.zeros((4, 4))\n",
    "sb = 0\n",
    "mc = np.mean(data, axis=0)\n",
    "\n",
    "C = np.cov(np.transpose(data))\n",
    "# Loop over classes\n",
    "classes = np.unique(labels)\n",
    "for i in range(len(classes)):\n",
    "# Find relevant datapoints\n",
    "    indices = np.squeeze(np.where(labels==classes[i]))\n",
    "    d = np.squeeze(data[indices,:])\n",
    "    classcov = np.cov(d.T)\n",
    "    Sw += np.float(np.shape(indices)[0])/Ndata * classcov\n",
    "    m = np.mean(d, axis=0)\n",
    "    sb += np.dot( (m-mc).reshape(4,1) , (m-mc).reshape(1,4) )/3\n",
    "    print ('Sw: \\n',Sw)\n",
    "\n",
    "print('sb\\n',sb,'\\n')\n",
    "print(C - Sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our dataset is easy to separate into classes, then this within-class scatter should be\n",
    "small, so that each class is tightly clustered together. However, to be able to separate the\n",
    "data, we also want the distance between the classes to be large. This is known as the **between-\n",
    "classes scatter** and is a significantly simpler computation, simply looking at the difference\n",
    "in the means:\n",
    "\n",
    "\\begin{equation}\n",
    "S_B = \\sum_{c \\in classes} (\\mu_c - \\mu)(\\mu_c - \\mu)^T .\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sb = C - Sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument about good separation suggests that datasets that are easy to separate into the different classes\n",
    "should have  $S_B /S_W$ as large as possible.\n",
    "\n",
    "The projection of\n",
    "the data can be written as $z = w^T \\cdot x$ for datapoint $x$. \n",
    "\n",
    "Replacing $x_j$ with $w^T \\cdot x_j$, we can use some linear algebra to get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{c \\in classes} \\sum_{j \\in c}\n",
    "p_c(w^T \\cdot (x_j - \\mu_c))(w^{T} \\cdot (x_j - \\mu_c))^T = w^T S_W w\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{c \\in classes}\n",
    "w^T (\\mu_c - \\mu)(\\mu_c - \\mu)^Tw = w^T S_B w .\n",
    "\\end{equation}\n",
    "\n",
    "So our ratio of within-class and between-class scatter looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{w^T S_W w}{w^T S_B w}.\n",
    "\\end{equation}\n",
    "\n",
    "In order to find the maximum value of this with respect to $w$, we differentiate it and set\n",
    "the derivative equal to 0. This tells us that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{S_B w(w^TS_W w)-S_W w(w^T S_B w)}{(w^T S_W w)^2}S_B w = 0 .\n",
    "\\end{equation}\n",
    "\n",
    "So we just need to solve this equation for w and we are done. We start with a little bit\n",
    "of rearranging to get:\n",
    "\n",
    "\\begin{equation}\n",
    "S_Ww = \\frac{w^T S_W w}{w^T S_B w}S_B w.\n",
    "\\end{equation}\n",
    "\n",
    "Finding the minimum requires computing the generalised eigenvectors of $S_W^{-1} S_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals,evecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))\n",
    "print np.linalg.inv(Sw).dot(Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print evecs.dot(np.linalg.inv(Sw).dot(Sb)).dot(evecs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print evecs.T.dot(Sb).dot(evecs)/evecs.T.dot(Sw).dot(evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals,evecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))\n",
    "indices = np.argsort(evals)\n",
    "indices = indices[::-1]\n",
    "evecs = evecs[:,indices]\n",
    "evals = evals[indices]\n",
    "w = evecs[:,:2]\n",
    "n = np.dot(data,w)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for i, c,label in zip(target_ids, colors, iris.target_names):\n",
    "    plt.scatter(n[i == labels, 0], n[i == labels, 1], c = c, edgecolors='black', s=235,label=label, marker='.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "lda = lda(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)\n",
    "for i, c, label in zip(target_ids, colors,iris.target_names):\n",
    "    plt.scatter(X_lda[i == labels, 0], X_lda[i == labels, 1], c = c, edgecolors='black', s=235,label=label, marker='.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
