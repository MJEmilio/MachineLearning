{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Decision Trees\n",
    "\n",
    "### Entropy in Information Theory\n",
    "\n",
    "The entropy $H$ of a set of probabilities $p_i$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "Entropy(p) = - \\sum_i{p_i \\log_2 p_i}\n",
    "\\end{equation}\n",
    "\n",
    "where the logarithm is base 2 because we are imagining that we encode everything using binary digits (bits). The basic concept is that it tells us how much extra information we would get from knowing the value of that feature. A function for computing the entropy is very simple, as here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(p):\n",
    "    if p!=0:\n",
    "        return -p * np.log2(p)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "\n",
    "The important idea is to work out how much the entropy of the whole training set would decrease if we choose each particular feature for the next classification step. This is known as the __information gain__, and it is defined as the entropy of the whole set minus the entropy when a particular feature is chosen. This is defined by (where $S$ is the set of examples, $F$ is a possible feature out of the set of all possible ones, and $|S_f|$ is a count of the number of members of $S$ that have value $f$ for feature $F$ ):\n",
    "\n",
    "\\begin{equation}\n",
    "Gain(S,F)= Entropy(S)- \\sum_{f \\in values(F)} \\frac{\\mid S_f \\mid}{\\mid S \\mid}\n",
    "Entropy(S_f). \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code from Chapter 12 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class dtree:\n",
    "\t\"\"\" A basic Decision Tree\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\" Constructor \"\"\"\n",
    "\n",
    "\tdef read_data(self,filename):\n",
    "\t\tfid = open(filename,\"r\")\n",
    "\t\tdata = []\n",
    "\t\td = []\n",
    "\t\tfor line in fid.readlines():\n",
    "\t\t\td.append(line.strip())\n",
    "\t\tfor d1 in d:\n",
    "\t\t\tdata.append(d1.split(\",\"))\n",
    "\t\tfid.close()\n",
    "\n",
    "\t\tself.featureNames = data[0]\n",
    "\t\tself.featureNames = self.featureNames[:-1]\n",
    "\t\tdata = data[1:]\n",
    "\t\tself.classes = []\n",
    "\t\tfor d in range(len(data)):\n",
    "\t\t\tself.classes.append(data[d][-1])\n",
    "\t\t\tdata[d] = data[d][:-1]\n",
    "\n",
    "\t\treturn data,self.classes,self.featureNames\n",
    "\n",
    "\tdef classify(self,tree,datapoint):\n",
    "\n",
    "\t\tif type(tree) == type(\"string\"):\n",
    "\t\t\t# Have reached a leaf\n",
    "\t\t\treturn tree\n",
    "\t\telse:\n",
    "\t\t\ta = tree.keys()[0]\n",
    "\t\t\tfor i in range(len(self.featureNames)):\n",
    "\t\t\t\tif self.featureNames[i]==a:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\tt = tree[a][datapoint[i]]\n",
    "\t\t\t\treturn self.classify(t,datapoint)\n",
    "\t\t\texcept:\n",
    "\t\t\t\treturn None\n",
    "\n",
    "\tdef classifyAll(self,tree,data):\n",
    "\t\tresults = []\n",
    "\t\tfor i in range(len(data)):\n",
    "\t\t\tresults.append(self.classify(tree,data[i]))\n",
    "\t\treturn results\n",
    "\n",
    "\tdef make_tree(self,data,classes,featureNames,maxlevel=-1,level=0,forest=0):\n",
    "\t\t\"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "\t\tnData = len(data)\n",
    "\t\tnFeatures = len(data[0])\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\tself.featureNames\n",
    "\t\texcept:\n",
    "\t\t\tself.featureNames = featureNames\n",
    "\t\t\t\n",
    "\t\t# List the possible classes\n",
    "\t\tnewClasses = []\n",
    "\t\tfor aclass in classes:\n",
    "\t\t\tif newClasses.count(aclass)==0:\n",
    "\t\t\t\tnewClasses.append(aclass)\n",
    "\n",
    "\t\t# Compute the default class (and total entropy)\n",
    "\t\tfrequency = np.zeros(len(newClasses))\n",
    "\n",
    "\t\ttotalEntropy = 0\n",
    "\t\ttotalGini = 0\n",
    "\t\tindex = 0\n",
    "\t\tfor aclass in newClasses:\n",
    "\t\t\tfrequency[index] = classes.count(aclass)\n",
    "\t\t\ttotalEntropy += self.calc_entropy(float(frequency[index])/nData)\n",
    "\t\t\ttotalGini += (float(frequency[index])/nData)**2\n",
    "\n",
    "\t\t\tindex += 1\n",
    "\n",
    "\t\ttotalGini = 1 - totalGini\n",
    "\t\tdefault = classes[np.argmax(frequency)]\n",
    "\n",
    "\t\tif nData==0 or nFeatures == 0 or (maxlevel>=0 and level>maxlevel):\n",
    "\t\t\t# Have reached an empty branch\n",
    "\t\t\treturn default\n",
    "\t\telif classes.count(classes[0]) == nData:\n",
    "\t\t\t# Only 1 class remains\n",
    "\t\t\treturn classes[0]\n",
    "\t\telse:\n",
    "\n",
    "\t\t\t# Choose which feature is best\t\n",
    "\t\t\tgain = np.zeros(nFeatures)\n",
    "\t\t\tggain = np.zeros(nFeatures)\n",
    "\t\t\tfeatureSet = range(nFeatures)\n",
    "\t\t\tif forest != 0:\n",
    "\t\t\t\tnp.random.shuffle(featureSet)\n",
    "\t\t\t\tfeatureSet = featureSet[0:forest]\n",
    "\t\t\tfor feature in featureSet:\n",
    "\t\t\t\tg,gg = self.calc_info_gain(data,classes,feature)\n",
    "\t\t\t\tgain[feature] = totalEntropy - g\n",
    "\t\t\t\tggain[feature] = totalGini - gg\n",
    "\n",
    "\t\t\tbestFeature = np.argmax(gain)\n",
    "\t\t\ttree = {featureNames[bestFeature]:{}}\n",
    "\n",
    "\t\t\t# List the values that bestFeature can take\n",
    "\t\t\tvalues = []\n",
    "\t\t\tfor datapoint in data:\n",
    "\t\t\t\tif datapoint[feature] not in values:\n",
    "\t\t\t\t\tvalues.append(datapoint[bestFeature])\n",
    "\n",
    "\t\t\tfor value in values:\n",
    "\t\t\t\t# Find the datapoints with each feature value\n",
    "\t\t\t\tnewData = []\n",
    "\t\t\t\tnewClasses = []\n",
    "\t\t\t\tindex = 0\n",
    "\t\t\t\tfor datapoint in data:\n",
    "\t\t\t\t\tif datapoint[bestFeature]==value:\n",
    "\t\t\t\t\t\tif bestFeature==0:\n",
    "\t\t\t\t\t\t\tnewdatapoint = datapoint[1:]\n",
    "\t\t\t\t\t\t\tnewNames = featureNames[1:]\n",
    "\t\t\t\t\t\telif bestFeature==nFeatures:\n",
    "\t\t\t\t\t\t\tnewdatapoint = datapoint[:-1]\n",
    "\t\t\t\t\t\t\tnewNames = featureNames[:-1]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnewdatapoint = datapoint[:bestFeature]\n",
    "\t\t\t\t\t\t\tnewdatapoint.extend(datapoint[bestFeature+1:])\n",
    "\t\t\t\t\t\t\tnewNames = featureNames[:bestFeature]\n",
    "\t\t\t\t\t\t\tnewNames.extend(featureNames[bestFeature+1:])\n",
    "\t\t\t\t\t\tnewData.append(newdatapoint)\n",
    "\t\t\t\t\t\tnewClasses.append(classes[index])\n",
    "\t\t\t\t\tindex += 1\n",
    "\n",
    "\t\t\t\t# Now recurse to the next level\t\n",
    "\t\t\t\tsubtree = self.make_tree(newData,newClasses,newNames,maxlevel,level+1,forest)\n",
    "\n",
    "\t\t\t\t# And on returning, add the subtree on to the tree\n",
    "\t\t\t\ttree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "\t\t\treturn tree\n",
    "\n",
    "\tdef printTree(self,tree,name):\n",
    "\t\tif type(tree) == dict:\n",
    "\t\t\tprint name, tree.keys()[0]\n",
    "\t\t\tfor item in tree.values()[0].keys():\n",
    "\t\t\t\tprint name, item\n",
    "\t\t\t\tself.printTree(tree.values()[0][item], name + \"\\t\")\n",
    "\t\telse:\n",
    "\t\t\tprint name, \"\\t->\\t\", tree\n",
    "\n",
    "\tdef calc_entropy(self,p):\n",
    "\t\tif p!=0:\n",
    "\t\t\treturn -p * np.log2(p)\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\tdef calc_info_gain(self,data,classes,feature):\n",
    "\n",
    "\t\t# Calculates the information gain based on both entropy and the Gini impurity\n",
    "\t\tgain = 0\n",
    "\t\tggain = 0\n",
    "\t\tnData = len(data)\n",
    "\n",
    "\t\t# List the values that feature can take\n",
    "\n",
    "\t\tvalues = []\n",
    "\t\tfor datapoint in data:\n",
    "\t\t\tif datapoint[feature] not in values:\n",
    "\t\t\t\tvalues.append(datapoint[feature])\n",
    "\n",
    "\t\tfeatureCounts = np.zeros(len(values))\n",
    "\t\tentropy = np.zeros(len(values))\n",
    "\t\tgini = np.zeros(len(values))\n",
    "\t\tvalueIndex = 0\n",
    "\t\t# Find where those values appear in data[feature] and the corresponding class\n",
    "\t\tfor value in values:\n",
    "\t\t\tdataIndex = 0\n",
    "\t\t\tnewClasses = []\n",
    "\t\t\tfor datapoint in data:\n",
    "\t\t\t\tif datapoint[feature]==value:\n",
    "\t\t\t\t\tfeatureCounts[valueIndex]+=1\n",
    "\t\t\t\t\tnewClasses.append(classes[dataIndex])\n",
    "\t\t\t\tdataIndex += 1\n",
    "\n",
    "\t\t\t# Get the values in newClasses\n",
    "\t\t\tclassValues = []\n",
    "\t\t\tfor aclass in newClasses:\n",
    "\t\t\t\tif classValues.count(aclass)==0:\n",
    "\t\t\t\t\tclassValues.append(aclass)\n",
    "\n",
    "\t\t\tclassCounts = np.zeros(len(classValues))\n",
    "\t\t\tclassIndex = 0\n",
    "\t\t\tfor classValue in classValues:\n",
    "\t\t\t\tfor aclass in newClasses:\n",
    "\t\t\t\t\tif aclass == classValue:\n",
    "\t\t\t\t\t\tclassCounts[classIndex]+=1 \n",
    "\t\t\t\tclassIndex += 1\n",
    "\t\t\t\n",
    "\t\t\tfor classIndex in range(len(classValues)):\n",
    "\t\t\t\tentropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex])/np.sum(classCounts))\n",
    "\t\t\t\tgini[valueIndex] += (float(classCounts[classIndex])/np.sum(classCounts))**2\n",
    "\n",
    "\t\t\t# Computes both the Gini gain and the entropy\n",
    "\t\t\tgain = gain + float(featureCounts[valueIndex])/nData * entropy[valueIndex]\n",
    "\t\t\tggain = ggain + float(featureCounts[valueIndex])/nData * gini[valueIndex]\n",
    "\t\t\tvalueIndex += 1\n",
    "\t\treturn gain, 1-ggain\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Party\n",
      "  Yes\n",
      " \t\t->\tParty\n",
      "  No\n",
      " \tDeadline\n",
      " \tUrgent\n",
      " \t\t\t->\tStudy\n",
      " \tNone\n",
      " \t\t\t->\tPub\n",
      " \tNear\n",
      " \t\tLazy\n",
      " \t\tYes\n",
      " \t\t\t\t->\tTV\n",
      " \t\tNo\n",
      " \t\t\t\t->\tStudy\n",
      "['Party', 'Study', 'Party', 'Party', 'Pub', 'Party', 'Study', 'TV', 'Party', 'Study']\n",
      "True Classes\n",
      "['Party', 'Study', 'Party', 'Party', 'Pub', 'Party', 'Study', 'TV', 'Party', 'Study']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Code from Chapter 12 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "# Code to run the decision tree on the Party dataset\n",
    "\n",
    "\n",
    "tree = dtree()\n",
    "party,classes,features = tree.read_data('party.data')\n",
    "t=tree.make_tree(party,classes,features)\n",
    "tree.printTree(t,' ')\n",
    "\n",
    "print tree.classifyAll(t,party)\n",
    "\n",
    "for i in range(len(party)):\n",
    "    tree.classify(t,party[i])\n",
    "\n",
    "\n",
    "print \"True Classes\"\n",
    "print classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
